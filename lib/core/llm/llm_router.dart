import 'package:flutter_riverpod/flutter_riverpod.dart';
import 'package:logger/logger.dart';

enum LLMModel {
  gemma1B,
  exaone24B,
  gpt4o,
}

enum LLMMode {
  offline,
  online,
  hybrid,
}

class LLMRequest {
  final String message;
  final String? context;
  final Map<String, dynamic>? metadata;
  final bool isMultimodal;
  final List<String>? imageUrls;

  const LLMRequest({
    required this.message,
    this.context,
    this.metadata,
    this.isMultimodal = false,
    this.imageUrls,
  });
}

class LLMResponse {
  final String response;
  final LLMModel usedModel;
  final double confidence;
  final Map<String, dynamic>? metadata;
  final DateTime timestamp;

  const LLMResponse({
    required this.response,
    required this.usedModel,
    required this.confidence,
    this.metadata,
    required this.timestamp,
  });
}

class LLMRouter {
  final Logger _logger = Logger();
  LLMMode _currentMode = LLMMode.hybrid;
  bool _isOnline = true;
  bool _isGemmaLoaded = false;
  bool _isExaoneLoaded = false;

  // Model availability status
  bool get isGemmaAvailable => _isGemmaLoaded;
  bool get isExaoneAvailable => _isExaoneLoaded;
  bool get isOnline => _isOnline;
  LLMMode get currentMode => _currentMode;

  void setMode(LLMMode mode) {
    _currentMode = mode;
    _logger.i('LLM mode changed to: $mode');
  }

  void setOnlineStatus(bool isOnline) {
    _isOnline = isOnline;
    _logger.i('Online status changed to: $isOnline');
  }

  void setModelStatus({bool? gemmaLoaded, bool? exaoneLoaded}) {
    if (gemmaLoaded != null) _isGemmaLoaded = gemmaLoaded;
    if (exaoneLoaded != null) _isExaoneLoaded = exaoneLoaded;
    _logger.i('Model status - Gemma: $_isGemmaLoaded, EXAONE: $_isExaoneLoaded');
  }

  Future<LLMResponse> processRequest(LLMRequest request) async {
    final selectedModel = _selectModel(request);
    _logger.i('Selected model: $selectedModel for request: ${request.message.substring(0, 50)}...');

    switch (selectedModel) {
      case LLMModel.gemma1B:
        return await _processWithGemma(request);
      case LLMModel.exaone24B:
        return await _processWithExaone(request);
      case LLMModel.gpt4o:
        return await _processWithGPT4o(request);
    }
  }

  LLMModel _selectModel(LLMRequest request) {
    // If offline mode is forced, use available offline models
    if (_currentMode == LLMMode.offline || !_isOnline) {
      // Prioritize Gemma3 for all offline queries
      if (_isGemmaLoaded) {
        return LLMModel.gemma1B;
      } else if (_isExaoneLoaded) {
        return LLMModel.exaone24B;
      } else {
        throw Exception('No offline models available');
      }
    }

    // If online mode is forced, use GPT-4o
    if (_currentMode == LLMMode.online) {
      return LLMModel.gpt4o;
    }

    // Hybrid mode - intelligent routing with Gemma3 preference
    if (request.isMultimodal) {
      return LLMModel.gpt4o; // Multimodal requires cloud model
    }

    // Use Gemma3 for most queries, only escalate to cloud for very complex ones
    if (_isVeryComplexQuery(request.message)) {
      return LLMModel.gpt4o; // Only very complex queries go to cloud
    }

    // Prefer Gemma3 for all text queries (including Korean)
    if (_isGemmaLoaded) {
      return LLMModel.gemma1B;
    }

    // Fallback to EXAONE if Gemma not available
    if (_isExaoneLoaded) {
      return LLMModel.exaone24B;
    }

    // Final fallback to cloud if no offline models available
    return LLMModel.gpt4o;
  }

  bool _isKoreanText(String text) {
    // Simple Korean character detection
    final koreanRegex = RegExp(r'[ã„±-ã…ê°€-í£]');
    final koreanMatches = koreanRegex.allMatches(text).length;
    return koreanMatches > text.length * 0.3; // 30% Korean characters
  }

  bool _isVeryComplexQuery(String text) {
    // Only very complex queries that really need GPT-4o
    final veryComplexIndicators = [
      'comprehensive analysis', 'ì¢…í•©ì  ë¶„ì„', 
      'detailed diagnosis', 'ì •ë°€ ì§„ë‹¨',
      'treatment plan', 'ì¹˜ë£Œ ê³„íš',
      'medical emergency', 'ì‘ê¸‰ìƒí™©',
      'drug interaction', 'ì•½ë¬¼ ìƒí˜¸ì‘ìš©'
    ];
    
    final lowerText = text.toLowerCase();
    return veryComplexIndicators.any((indicator) => lowerText.contains(indicator)) ||
           text.length > 500 || // Very long queries only
           text.split(' ').length > 50; // Very many words
  }

  bool _isComplexQuery(String text) {
    // Keep original method for other uses
    final complexIndicators = [
      'analyze', 'ë¶„ì„', 'compare', 'ë¹„êµ', 'explain', 'ì„¤ëª…',
      'diagnosis', 'ì§„ë‹¨', 'treatment', 'ì¹˜ë£Œ', 'medical', 'ì˜ë£Œ'
    ];
    
    final lowerText = text.toLowerCase();
    return complexIndicators.any((indicator) => lowerText.contains(indicator)) ||
           text.length > 200 || // Long queries
           text.split(' ').length > 30; // Many words
  }

  Future<LLMResponse> _processWithGemma(LLMRequest request) async {
    try {
      // Simulate Gemma processing
      await Future.delayed(const Duration(milliseconds: 500));
      
      final response = _generateBasicResponse(request.message, 'Gemma');
      
      return LLMResponse(
        response: response,
        usedModel: LLMModel.gemma1B,
        confidence: 0.8,
        timestamp: DateTime.now(),
        metadata: {'processing_time_ms': 500, 'model_version': 'gemma-1b-q4'},
      );
    } catch (e) {
      _logger.e('Gemma processing failed: $e');
      rethrow;
    }
  }

  Future<LLMResponse> _processWithExaone(LLMRequest request) async {
    try {
      // Simulate EXAONE processing
      await Future.delayed(const Duration(milliseconds: 800));
      
      final response = _generateBasicResponse(request.message, 'EXAONE');
      
      return LLMResponse(
        response: response,
        usedModel: LLMModel.exaone24B,
        confidence: 0.9,
        timestamp: DateTime.now(),
        metadata: {'processing_time_ms': 800, 'model_version': 'exaone-2.4b-q4'},
      );
    } catch (e) {
      _logger.e('EXAONE processing failed: $e');
      rethrow;
    }
  }

  Future<LLMResponse> _processWithGPT4o(LLMRequest request) async {
    try {
      // Simulate GPT-4o API call
      await Future.delayed(const Duration(milliseconds: 1500));
      
      final response = _generateAdvancedResponse(request.message);
      
      return LLMResponse(
        response: response,
        usedModel: LLMModel.gpt4o,
        confidence: 0.95,
        timestamp: DateTime.now(),
        metadata: {'processing_time_ms': 1500, 'model_version': 'gpt-4o'},
      );
    } catch (e) {
      _logger.e('GPT-4o processing failed: $e');
      rethrow;
    }
  }

  String _generateBasicResponse(String message, String modelName) {
    // Enhanced responses for offline models (especially Gemma3)
    final lowerMessage = message.toLowerCase();
    
    if (lowerMessage.contains('ì•ˆë…•') || lowerMessage.contains('hello')) {
      return 'ì•ˆë…•í•˜ì„¸ìš”! ${modelName == 'Gemma' ? 'Gemma3' : modelName} AI ê±´ê°• ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ğŸ¥\n\nê±´ê°•ê³¼ ê´€ë ¨ëœ ë‹¤ì–‘í•œ ì§ˆë¬¸ì„ ë„ì™€ë“œë¦´ ìˆ˜ ìˆì–´ìš”. ì‹ë‹¨, ìš´ë™, ìˆ˜ë©´ ë“± ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!';
    }
    
    if (lowerMessage.contains('ì‹ë‹¨') || lowerMessage.contains('ìŒì‹') || lowerMessage.contains('ì¹¼ë¡œë¦¬')) {
      return '${modelName == 'Gemma' ? 'Gemma3' : modelName}ê°€ ì‹ë‹¨ ì¡°ì–¸ì„ ë“œë¦½ë‹ˆë‹¤! ğŸ\n\nê· í˜•ì¡íŒ ì‹ë‹¨ì˜ ê¸°ë³¸ì›ì¹™:\nâ€¢ íƒ„ìˆ˜í™”ë¬¼ 50-60% (í˜„ë¯¸, ê·€ë¦¬ ë“±)\nâ€¢ ë‹¨ë°±ì§ˆ 15-20% (ìƒì„ , ë‹­ê°€ìŠ´ì‚´, ì½©ë¥˜)\nâ€¢ ì§€ë°© 20-30% (ê²¬ê³¼ë¥˜, ì˜¬ë¦¬ë¸Œì˜¤ì¼)\nâ€¢ ì¶©ë¶„í•œ ì±„ì†Œì™€ ê³¼ì¼\n\ní˜„ì¬ ë“œì‹œëŠ” ìŒì‹ì´ë‚˜ ëª©í‘œê°€ ìˆìœ¼ì‹œë©´ ë” êµ¬ì²´ì ì¸ ì¡°ì–¸ì„ ë“œë¦´ ìˆ˜ ìˆì–´ìš”!';
    }
    
    if (lowerMessage.contains('ìš´ë™') || lowerMessage.contains('í—¬ìŠ¤') || lowerMessage.contains('ê·¼ìœ¡')) {
      return '${modelName == 'Gemma' ? 'Gemma3' : modelName}ì˜ ìš´ë™ ê°€ì´ë“œì…ë‹ˆë‹¤! ğŸ’ª\n\nì´ˆë³´ì ì¶”ì²œ ê³„íš:\nâ€¢ ì£¼ 3-4íšŒ, 30-45ë¶„\nâ€¢ ìœ ì‚°ì†Œ: ë¹ ë¥¸ ê±·ê¸°, ì¡°ê¹…, ìˆ˜ì˜\nâ€¢ ê·¼ë ¥: ìŠ¤ì¿¼íŠ¸, í‘¸ì‹œì—…, í”Œë­í¬\nâ€¢ ì ì§„ì  ê°•ë„ ì¦ê°€\n\ní˜„ì¬ ìš´ë™ ê²½í—˜ì´ë‚˜ ëª©í‘œë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ë§ì¶¤ ê³„íšì„ ì œì•ˆí•´ë“œë¦´ê²Œìš”!';
    }
    
    if (lowerMessage.contains('ìˆ˜ë©´') || lowerMessage.contains('ì ') || lowerMessage.contains('ë¶ˆë©´')) {
      return '${modelName == 'Gemma' ? 'Gemma3' : modelName}ì˜ ìˆ˜ë©´ ê°œì„  ì¡°ì–¸ì…ë‹ˆë‹¤! ğŸ˜´\n\nì¢‹ì€ ìˆ˜ë©´ì„ ìœ„í•œ ìŠµê´€:\nâ€¢ ê·œì¹™ì ì¸ ìˆ˜ë©´ì‹œê°„ (7-8ì‹œê°„)\nâ€¢ ì·¨ì¹¨ 1ì‹œê°„ ì „ ë””ì§€í„¸ ê¸°ê¸° ê¸ˆì§€\nâ€¢ ì‹¤ë‚´ì˜¨ë„ 18-22Â°C ìœ ì§€\nâ€¢ ì¹´í˜ì¸ì€ ì˜¤í›„ 2ì‹œ ì´í›„ ê¸ˆì§€\nâ€¢ ê°€ë²¼ìš´ ìŠ¤íŠ¸ë ˆì¹­ì´ë‚˜ ëª…ìƒ\n\ní˜„ì¬ ìˆ˜ë©´ íŒ¨í„´ì— ë¬¸ì œê°€ ìˆìœ¼ì‹œë‹¤ë©´ ë” ìì„¸íˆ ìƒë‹´í•´ë“œë¦´ê²Œìš”!';
    }
    
    if (lowerMessage.contains('ìŠ¤íŠ¸ë ˆìŠ¤') || lowerMessage.contains('ìš°ìš¸') || lowerMessage.contains('ë¶ˆì•ˆ')) {
      return '${modelName == 'Gemma' ? 'Gemma3' : modelName}ê°€ ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬ë¥¼ ë„ì™€ë“œë¦½ë‹ˆë‹¤! ğŸ§˜â€â™€ï¸\n\níš¨ê³¼ì ì¸ ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬ë²•:\nâ€¢ ê·œì¹™ì ì¸ ìš´ë™ (ì—”ë„ë¥´í•€ ë¶„ë¹„)\nâ€¢ ì‹¬í˜¸í¡ê³¼ ëª…ìƒ (í•˜ë£¨ 10ë¶„)\nâ€¢ ì¶©ë¶„í•œ ìˆ˜ë©´ê³¼ íœ´ì‹\nâ€¢ ì·¨ë¯¸í™œë™ê³¼ ì‚¬íšŒì  ê´€ê³„\nâ€¢ ê¸ì •ì  ì‚¬ê³  í›ˆë ¨\n\nì§€ì†ì ì¸ ì¦ìƒì´ ìˆë‹¤ë©´ ì „ë¬¸ì˜ ìƒë‹´ë„ ê¶Œí•´ë“œë ¤ìš”.';
    }
    
    return '${modelName == 'Gemma' ? 'Gemma3' : modelName}ì…ë‹ˆë‹¤! ğŸ˜Š\n\në” êµ¬ì²´ì ì¸ ê±´ê°• ìƒë‹´ì„ ìœ„í•´ ë‹¤ìŒì„ ì•Œë ¤ì£¼ì„¸ìš”:\nâ€¢ í˜„ì¬ ìƒí™©ì´ë‚˜ ì¦ìƒ\nâ€¢ ëª©í‘œë‚˜ ê¶ê¸ˆí•œ ì \nâ€¢ ìƒí™œ íŒ¨í„´\n\nê°œì¸ ë§ì¶¤í˜• ì¡°ì–¸ì„ ë“œë¦´ ìˆ˜ ìˆë„ë¡ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤!';
  }

  String _generateAdvancedResponse(String message) {
    // More sophisticated responses for cloud model
    final lowerMessage = message.toLowerCase();
    
    if (lowerMessage.contains('ë¶„ì„') || lowerMessage.contains('analyze')) {
      return '''ê±´ê°• ë°ì´í„° ë¶„ì„ ê²°ê³¼ë¥¼ ë§ì”€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

í˜„ì¬ ì œê³µí•´ì£¼ì‹  ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•©ì ì¸ ë¶„ì„ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤:

1. **ì „ë°˜ì ì¸ ê±´ê°• ìƒíƒœ**: ì–‘í˜¸í•œ í¸ì´ì§€ë§Œ ëª‡ ê°€ì§€ ê°œì„ ì ì´ ìˆìŠµë‹ˆë‹¤.
2. **ì£¼ìš” ê´€ì‹¬ ì˜ì—­**: ì‹ë‹¨ ê´€ë¦¬ì™€ ìš´ë™ íŒ¨í„´ ìµœì í™”ê°€ í•„ìš”í•´ ë³´ì…ë‹ˆë‹¤.
3. **ê¶Œì¥ì‚¬í•­**: 
   - ê·œì¹™ì ì¸ ìƒí™œ íŒ¨í„´ ìœ ì§€
   - ê· í˜•ì¡íŒ ì˜ì–‘ ì„­ì·¨
   - ì ì ˆí•œ ìˆ˜ë¶„ ì„­ì·¨
   - ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬

ë” ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ì¶”ê°€ì ì¸ ê±´ê°• ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ë¶€ë¶„ì— ëŒ€í•œ ë¶„ì„ì„ ì›í•˜ì‹œë‚˜ìš”?''';
    }
    
    return '''GPT-4o ëª¨ë¸ì„ í†µí•´ ê³ ë„í™”ëœ ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤.

ê·€í•˜ì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ê°ë„ë¡œ ë¶„ì„í•œ ê²°ê³¼:

â€¢ **ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì¹˜**: í˜„ì¬ ìƒí™©ì—ì„œ ë°”ë¡œ ì ìš©í•  ìˆ˜ ìˆëŠ” ë°©ë²•ë“¤ì„ ì œì•ˆí•©ë‹ˆë‹¤.
â€¢ **ì¤‘ì¥ê¸° ê³„íš**: ì§€ì†ì ì¸ ê±´ê°• ê´€ë¦¬ë¥¼ ìœ„í•œ ë‹¨ê³„ë³„ ì ‘ê·¼ë²•ì„ ì•ˆë‚´í•©ë‹ˆë‹¤.
â€¢ **ê°œì¸ ë§ì¶¤í˜• ê¶Œì¥ì‚¬í•­**: ê·€í•˜ì˜ íŠ¹ì„±ì— ë§ëŠ” êµ¬ì²´ì ì¸ ê°€ì´ë“œë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤.

ì¶”ê°€ì ì¸ ì •ë³´ë‚˜ ë” êµ¬ì²´ì ì¸ ìƒë‹´ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”.''';
  }
}

// Riverpod providers
final llmRouterProvider = Provider<LLMRouter>((ref) {
  return LLMRouter();
});

final llmModeProvider = StateProvider<LLMMode>((ref) {
  return LLMMode.hybrid;
});

final llmOnlineStatusProvider = StateProvider<bool>((ref) {
  return true;
});

